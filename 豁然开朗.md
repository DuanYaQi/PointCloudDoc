# 豁然开朗

machine independent 机器无关 即不受机器的影响



## PointNet

Inspired by PointNet, we directly process 3D points by converting their coordinates into deep features and producing more points by feature expansion . 

 受PointNet的启发，我们通过将3D点的**坐标转换为深层特征**并通过**特征扩展**产生更多点来直接处理3D点。



## 深度学习恒等映射Res-Net

​	赋予神经网络无限可能性的“非线性”让神经网络模型走得太远，却也让它忘记了为什么出发（想想还挺哲学）。这也使得特征随着层层前向传播得到完整保留（什么也不做）的可能性都微乎其微。

用学术点的话说，这种神经网络丢失的“不忘初心”/“什么都不做”的品质叫做**恒等映射（identity mapping）**。

​	因此，可以认为Residual Learning的初衷，其实是让模型的内部结构至少有恒等映射的能力。以保证在堆叠网络的过程中，网络至少不会因为继续堆叠而产生退化！



## ill-posed problem

​	Point cloud upsampling is an **ill-posed problem** due to the uncertainty or ambiguity of upsampled point clouds. Given a sparse input point cloud, there are many feasible output point distributions. Therefore, we do not have the notion of **“correct pairs”** of input and ground truth. 

​	由于上采样点云的不确定性或歧义性，点云上采样是一个不适定的问题。 给定稀疏的输入点云，存在许多可行的输出点分布。 因此，我们没有输入和 G.T. 的“正确对”的概念。   即没有一一对应映射的性质。

解决方法：on-the-ﬂy input generation scheme 即时输入生成方案





## 特征

​	理论上讲越复杂的特征有越强的表征能力



## 点云上采样

This upsampling problem is similar in spirit to the image **super-resolution** problem [33, 20]; however, dealing with **3D points** rather than a **2D grid of pixels**
poses new challenges.  上采样问题在本质上与图像超分辨率问题相似[33，20]。但是，处理3D点而不是2D像素网格带来了新的挑战。



we expand **the number of features** in the **feature space**. This is equivalent to expanding the **number of points**, since points and features are **interchangeable**. 我们扩展了特征空间中的特征数量，这相当于扩展点的数量，因为点和特征是可以互换的。



Most deep upsampling pipelines comprise two major components: **feature extraction** and **point upsampling**. The performance of the point upsampling component tends to deﬁne the eﬀectiveness of the ﬁnal network.

大多数深度上采样管线包括两个主要组件：特征提取和点上采样。点上采样组件的性能倾向于定义最终网络的有效性。



Point clouds often represent objects with variable part sizes. Using **multi-scale features** is an eﬀective way to encode this property, and is essential for obtaining point clouds of high quality. 

点云通常代表具有可变零件尺寸的对象。使用**多尺度特征**是编码此属性的有效方法，并且对于获得高质量的点云至关重要。

 

downsampling leads to loss of ﬁne-grained details

下采样会导致细粒度细节的丢失



## 点云生成

CVPR 2020_PointGMM

​	可以使用 VAE 框架训练 PointGMM 以进行形状生成。本文编码器采用 PointNet 架构。PointNet 通过在每个点上使用 MLP 对给定的点云 $X∈R^{N×d}$ 进行编码。PointNet 通过在每个维上应用全局 max pooling 运算符来维持阶数不变性，从而产生阶数不变嵌入 $\hat{Z}$

​	这个嵌入包含参数 $Z_μ$ 和 $Z_σ$  ，用于生成潜矢量 $Z=Z_μ+ϵZ_σ$，其中 $ϵ∼N(0,1)$。这实现了**可以随机采样的平滑的潜在空间**，从而使我们的**模型具有生成能力**。





## feature expansion

​	其实PU-GAN中generator的整体框架还是PU-Net那一套：**patch --> feature extraction --> feature expansion --> coordinate reconstruction**



​	和PU-Net一样，PU-GAN也设计了自己的feature expansion模块，这也应该是上采样算法的核心了吧



## 拓扑

​	Inspired by the dynamic graph convolution [46, 52], we deﬁne our local neighborhood in feature space. 

​	受动态图卷积的启发[46，52]，我们定义了特征空间中的局部邻域。



​	图把没有拓扑结构的点云变得像二维图片一样具有邻域，即有确定的邻域关系/相对关系。通过边连接



## 不规则拓扑

There are tremendous amount of works on applying deep learning onto irregular data such as graphs and pointsets[2,5,6,12,14,15,23,24,28,32,35,38,39,43,47,52, 59].

将深度学习应用于图形和点集之类的不规则数据方面有大量工作[2,5,6,12,14,15,23,24,28,32,35,38,39,43,47,52,59]。





## kNN

​	In contrast to images, point sets do not have the regular structure, and the neighborhoods of points are not ﬁxed sets. **Neighborhood information** must be collected by, e.g., k-nearest neighbors (kNN) search. 

​	图像相比，点集没有规则的结构，点的邻域也不是固定集。 邻居信息必须通过例如k近邻（kNN）搜索来收集。 





## CNN

​	traditional convolutional neural network (CNN) requires its neighboring samples to appear at some **ﬁxed spatial orientations and distances** so as to facilitate the **convolution**. 

​	传统的卷积神经网络（CNN）都要求其邻近样本出现在某些固定的空间方向和距离上，以便于卷积。



---

## GCN

GCNs are considered a tool to process non-Euclidean data. 

GCN被认为是处理非欧几里得数据的工具。





----

## skip-attention

​	Point cloud completion aims to infer the complete geometries for missing regions of 3D objects from incomplete ones. Previous methods usually predict the complete point cloud based on the global shape representation extracted from the incomplete input. However, the global representation often suffers from the information loss of structure details on local regions of incomplete point cloud. To address this problem, we propose Skip-Attention Network(SA-Net) for 3D point cloud completion. 

​	点云补全的目的是从不完整的对象中推断出3D对象缺失区域的完整对象。先前的方法通常基于从不完整输入中提取的全局形状表示来预测完整点云。但是，全局表示通常会遭受不完整点云局部区域上结构细节的信息丢失。 为了解决这个问题，我们提出了**跳过注意力**网络（SA-Net）来完成3D点云。





---

## 先验

In the past two decades, a wide range of techniques have been developed to address this problem, including denoising, completion, resampling, and many more. 

在过去的二十年中，已经开发了各种各样的技术来解决此问题，包括降噪，完成，重采样等等。

However, these techniques are mostly based on **priors**, such as piecewise smoothness. 

但是，这些技术主要基于先验条件，例如分段平滑度。

Priors are typically **over-simpliﬁed models** of the **actual geometry behavior**, thus the prior-based techniques tend to work well for speciﬁc class of models rather than being general.

先验通常是**实际几何行为的过度简化模型**，因此，**基于先验的技术**对于**特定类型**的模型趋向于很好地工作，而**不是通用的**。





---

## 显式和隐式

人工智能强智能必须使用**隐式模型**来构造。


显式模型特点就是：**数据和逻辑都被写死固定的、规定好的**。一般程序都是这个路子。


但是强智能不能这么搞，强智能需要建立在一个更为底层的系统之中，让数据和逻辑从这个结构中生成。


不仅仅它的对象和方法，包括它的主体，都必须是它的底层系统来运行并且生成的，生成之后，接管控制权。


为了创造一个世界，必须创造生成这个世界的世界。  



任何高级概念都可以用基本概念堆叠而成。比如程序三顺序结构可幻化任意程序流程。然而没有基本就无法堆叠。

你这篇还是模拟脑的思路。你觉的脑子里本没有数，人研究了现象才发明了数。
数是脑适应现实世界的一种子程序。

然而大多数人都是直接学习得到得数。也就是说，可以从底层发现生成，也可以直接输入。  





## 随机性

 we randomly select $\hat{N}=1024$ points out of the 2048 points to introduce randomness into the point distribution, and normalize the 3D coordinates of the points to have zero mean inside a unit ball

我们从2048个点中随机选择 $\hat{N}=1024$ 个点，以将随机性引入点分布，并将这些点的3D坐标归一化以使单位球内的均值为零





## 回归

​	In addition, to train the network to be edge-aware, we associate edge and mesh triangle information with the training patches, and train the network to learn features from the patches by regressing point-to-edge distances and then the point coordinates. 

​	此外，为了使网络具有边缘感知能力，我们将边缘和网格三角形信息与训练块相关联，并通过回归得到点到边缘的距离，然后再回归得到点的坐标，从而训练网络从块中学习特征。 



C.R.Rao等在Linear Models and Generalizations: Least Squares and Alternatives中解释道 the literature meaning of REGRESSION is " to move in the backward direction"，看以下两个陈述：
	S1: model generates data or
	S2: data generates model.

​	Rao认为很明显陈述S1才是对的，因为模型实际上本来就是存在的只不过我们不知道(model exists in nature but is unknown to the experimenter)，先有模型所以我们知道X就能得到Y：

​	先有模型 =》有了X就有Y（S1）

而“回归”的意思就是我们通过收集X与Y来确定实际上存在的关系模型：

​	收集X与Y =》确定模型（S2）

与S1相比，S2就是一个“回到”模型的过程，所以就叫做“regression”。



## 极限

当考虑一个数为无穷大或无穷小，比较抽象。

考虑一个你能把握的且合理的较大或较小的数也可以帮助理解。