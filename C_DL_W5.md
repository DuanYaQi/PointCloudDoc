# 深度学习工程师

由 deeplearning.ai 出品，网易引进的正版授权中文版深度学习工程师微专业课程，让你在了解丰富的人工智能应用案例的同时，学会在实践中搭建出最先进的神经网络模型，训练出属于你自己的 AI。



deeplearning.ai

https://www.coursera.org/learn/neural-networks-deep-learning?action=enroll

https://study.163.com/my#/smarts

https://www.bilibili.com/video/av66647398



**note**

https://redstonewill.blog.csdn.net/article/details/79446105

http://www.ai-start.com/dl2017/



**课后作业**

https://blog.csdn.net/u013733326/article/details/79827273

https://www.heywhale.com/mw/project/5e20243e2823a10036b542da





## Question

- [ ] 特殊应用：人脸识别和神经风格转换-[4.7 CNN可视化解释](#4.7)，伪影





## 序列模型

### 第一周 循环序列模型

#### 1.1 为什么选择序列模型

![这里写图片描述](assets/20180305154333482)

- **语音识别**：给定输入音频片段 X，输出一段文字 Y。输入输出都是序列模型，因为 X 是一个按时序播放的音频片段，Y 是一系列单词。
- **音乐发生器**：输入数据 X 可以是空集，也可以是单一的整数（表示音乐风格），也可能是生成的前几个音符。输出音乐序列 Y。
- **情感分类**：输入 X 是文字序列，输出评价指标。
- **DNA序列分析**：输入**DNA**序列 A、C、G、T 组成，输出各部分匹配的某种蛋白质。
- **机器翻译**：输入句子，输出另一种语言的翻译结果。
- **视频动作识别**：输入视频帧，输出识别其中的行为。
- **命名实体识别**：输入句子，暑促识别出句中的人名。

这些序列模型基本都属于监督式学习，输入 X 和输出 Y 不一定都是序列模型。如果都是序列模型的话，模型长度不一定完全一致。



---

#### 1.2 数学符号

下面以命名实体识别为例，介绍序列模型的命名规则。示例语句为：

**Harry Potter and Hermione Granger invented a new spell.**

输入 x 包含 9 个单词，输出 y 即为 1 x 9 向量，每位表征对应单词是否为人名的一部分，1 表示是，0 表示否。很明显，该句话中“Harry”，“Potter”，“Hermione”，“Granger”均是人名成分，所以，对应的输出y可表示为：

$$
y=[1\quad 1\quad  0\quad  1\quad  1\quad  0\quad  0\quad  0\quad  0]
$$
一般约定使用 $y^{<t>}$ 表示序列对应位置的输出，使用 $T_y$ 表示输出序列长度，则 $1≤t≤T_y$。

对于输入 x，表示为：
$$
[x^{<1>}\quad x^{<2>}\quad x^{<3>}\quad x^{<4>}\quad x^{<5>}\quad x^{<6>}\quad x^{<7>}\quad x^{<8>}\quad x^{<9>}]
$$

同样， $x^{<t>}$ 表示序列对应位置的输入，$T_x$ 表示输入序列长度。注意，此例中，$T_x=T_y=9$，但是也存在 $T_x≠T_y$ 的情况。

如何来表示每个 $x^{<t>}$ 呢？方法是首先建立一个词汇库 vocabulary，尽可能包含更多的词汇。例如一个包含 10000 个词汇的词汇库为：

$$
\left[
\begin{matrix}
a \\
and \\
\cdot \\
\cdot \\
\cdot \\
harry \\
\cdot \\
\cdot \\
\cdot \\
potter \\
\cdot \\
\cdot \\
\cdot \\
zulu
\end{matrix}
\right]
$$
该词汇库可看成是 10000 x 1 的向量。值得注意的是自然语言处理 NLP 实际应用中的词汇库可达百万级别的词汇量。

然后，使用 one-hot 编码，例句中的每个单词 $x^{<t>}$ 都可以表示成10000 x 1的向量，词汇表中与 $x^{<t>}$ 对应的位置为 1，其它位置为 0。该 $x^{<t>}$ 为one-hot向量。值得一提的是如果出现词汇表之外的单词，可以使用 **UNK** （**Unknow Word**）或其他字符串来表示。

对于多样本，以上序列模型对应的命名规则可表示为：$x^{(i)<t>}$，$y^{(i)<t>}$，$T_x^{(i)}$，$T_y^{(i)}$。其中，$i$ 表示第 $i$ 个样本。不同样本的 $T_x^{(i)}$ 或 $T_y^{(i)}$ 都有可能不同。


---

#### 1.3 循环神经网络模型

对于序列模型，如果使用标准的神经网络，其模型结构如下：

![这里写图片描述](assets/20180305180556590)

使用标准的神经网络模型存在两个问题：

第一个问题，**不同样本的输入序列长度或输出序列长度不同**，即 $T_x^{(i)}\neq T_x^{(j)}$，$T_y^{(i)}\neq T_y^{(j)}$，造成模型难以统一。解决办法之一是设定一个最大序列长度，对每个输入和输出序列**补零**并统一到最大长度。但是这种做法实际效果并不理想。

第二个问题，也是主要问题，这种标准神经网络结构并**不共享**从文本的不同位置上**学到的特征**。例如，如果 $x^{<1>}$ 是“Harry”是人名成分，我们希望当句子其它位置 $x^{<5>}$ 出现了 “Harry”，可以用到位置 1 已经学到的特征将它识别出来，这是**共享特征**的结果，如同 CNN 网络特点一样（将部分图片里学到的内容快速推广到图片的其他部分）。但是，上图所示的网络不具备共享特征的能力。值得一提的是，共享特征还有助于减少神经网络中的参数数量，一定程度上减小了模型的计算复杂度。例如上图所示的标准神经网络，假设每个 $x^{<t>}$ 扩展到最大序列长度为 100，且词汇表长度为 10000，则输入层就已经包含了 100 x 10000 个神经元了，权重参数很多，运算量将是庞大的。

标准的神经网络不适合解决序列模型问题，而循环神经网络（RNN）是专门用来解决序列模型问题的。RNN 模型结构如下：

![这里写图片描述](assets/20180305203908747)

序列模型从左到右，依次传递，此例中， $T_x= T_y$。 $x^{<t>}$ 到 $y^{<t>}$ 之间是隐藏神经元。$a^{<t>}$ 会传入到第 $t+1$ 个元素中，作为输入。其中，$a^{<0>}$ 一般为零向量。

> 如果从左到右的顺序读这个句子，将第一个词 $x^{<1>}$ 输入一个神经网络层，可以让神经网络尝试预测输出 $\hat y^{<1>}$，判断这是否是人名的一部分。当读到句中的第二个单词 $x^{<2>}$ 时，它不是仅用 $x^{<2>}$ 就预测出 $\hat y^{<2>}$，它也会输入一些来自时间步 1 的信息。具体而言，时间步 1 的激活值就会传递到时间步 2。然后，在下一个时间步，循环神经网络输入了单词 $x^{<3>}$ ，然后它尝试预测输出了预测结果 $\hat y^{<3>}$，等等，一直到最后一个时间步，输入 $x^{<T_x>}$ ，然后输出 $\hat y^{<T_y>}$。
>
> 如果 $T_x$ 和 $T_y$ 不相同，结构会需要作出一些改变。所以**在每一个时间步中，循环神经网络传递一个激活值到下一个时间步中用于计算**。

RNN模型包含三类权重系数，分别是 $W_{ax}$，$W_{aa}$，$W_{ya}$。且不同元素之间同一位置共享同一权重系数。$W_ax$ 来表示管理着从 $x^{<1>}$ 到隐藏层的连接的一系列参数，每个时间步使用的都是**相同的参数** $W_ax$。而激活值也就是水平联系是由参数 $W_{aa}$ 决定的，同时每一个时间步都使用**相同的参数** $W_{aa}$，同样的输出结果由 $W_{ya}$ 决定 。

![这里写图片描述](assets/20180305212325555)

RNN的正向传播（Forward Propagation）过程为：

$$
a^{<t>}=g(W_{aa}\cdot a^{<t-1>}+W_{ax}\cdot x^{<t>}+ba)\\
\hat y^{<t>}=g(W_{ya}\cdot a^{<t>}+b_y)
$$
其中，$g(⋅)$ 表示激活函数，不同的问题需要使用不同的激活函数。

为了简化表达式，可以对 $a^{<t>}$ 项进行整合：

$$
W_{aa}\cdot a^{<t-1>}+W_{ax}\cdot x^{<t>}=[W_{aa}\ \ W_{ax}]\left[
\begin{matrix}
a^{<t-1>} \\
x^{<t>}
\end{matrix}
\right]\rightarrow W_a[a^{<t-1>},x^{<t>}]
$$
则正向传播可表示为：
$$
a^{<t>}=g(W_a[a^{<t-1>},x^{<t>}]+b_a)\\
\hat y^{<t>}=g(W_{y}\cdot a^{<t>}+b_y)
$$
值得一提的是，以上所述的RNN为单向RNN，即按照从左到右顺序，单向进行，$\hat y^{<t>}$ 只与左边的元素有关。但是，有时候 $\hat y^{<t>}$ 也可能与右边元素有关。例如下面两个句子中，单凭前三个单词，无法确定 “Teddy” 是否为人名，必须根据右边单词进行判断。

He said, “Teddy Roosevelt was a great President.”

He said, “Teddy bears are on sale!”

因此，有另外一种RNN结构是双向RNN，简称为BRNN。$\hat y^{<t>}$ 与左右元素均有关系，我们之后再详细介绍。



----

#### 1.4 通过时间的反向传播





#### 1.5 不同类型的循环神经网络





#### 1.6 语言模型和序列生成





#### 1.7 对新序列采样





#### 1.8 带有神经网络的梯度消失





#### 1.9 GRU 单元





#### 1.10 长短期记忆（LSTM）





#### 1.11 双向神经网络





#### 1.12 深层循环神经网络









### 第二周 自然语言处理与词嵌入

#### 2.1 词汇表征



#### 2.2 使用词嵌入



#### 2.3 词嵌入的特性



#### 2.4 嵌入矩阵



#### 2.5 学习词嵌入



#### 2.6 Word2Vec



#### 2.7 负采样



#### 2.8 GloVe 词向量



#### 2.9 情绪分类



#### 2.10 词嵌入除偏









### 第三周 序列模型和注意力机制

#### 3.1 基础模型





#### 3.2 选择最可能的句子





#### 3.3 定向搜索





#### 3.4 改进定向搜索





#### 3.5 定向搜索的误差分析





#### 3.6 Bleu 得分（选修）





#### 3.7 注意力模型直观理解





#### 3.8 注意力模型





#### 3.9 语音辨识





#### 3.10 触发字检测





#### 3.11 结论和致谢



