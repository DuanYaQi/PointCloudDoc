# æ·±åº¦ç”Ÿæˆæ¨¡å‹ç»¼è¿°

è‡ªåŠ¨åŒ–å­¦æŠ¥ æ·±åº¦ç”Ÿæˆæ¨¡å‹ç»¼è¿° èƒ¡é“­è²



æµæ¨¡å‹çš„åŸºæœ¬æ€æƒ³æ˜¯ï¼š**çœŸå®æ•°æ®åˆ†å¸ƒä¸€å®šå¯ä»¥ç”±è½¬æ¢å‡½æ•°æ˜ å°„åˆ°äººä¸ºç»™å®šçš„ç®€å•åˆ†å¸ƒ**ï¼Œå¦‚æœè¯¥**è½¬æ¢å‡½æ•°æ˜¯å¯é€†çš„**ä¸”å¯æ±‚å‡ºè¯¥è½¬æ¢å‡½æ•°çš„å½¢å¼ï¼Œåˆ™è¿™ä¸ª**ç®€å•åˆ†å¸ƒå’Œè½¬æ¢å‡½æ•°çš„é€†å‡½æ•°å°±èƒ½å¤Ÿæ„æˆä¸€ä¸ªæ·±åº¦ç”Ÿæˆæ¨¡å‹**ã€‚å¯é€†å‡½æ•°çš„æ€§è´¨è¯´æ˜ Flow æ¨¡å‹æ˜¯ä¸€ä¸ªç²¾ç¡®æ¨¡å‹ï¼Œæœ‰å¸Œæœ›ç”Ÿæˆè´¨é‡è¶³å¤Ÿå¥½çš„æ ·æœ¬ã€‚

Flow æ¨¡å‹çš„ç›¸å…³è®ºæ–‡è¾ƒå°‘ï¼Œé‡è¦çš„è®ºæ–‡ä¸­å­˜åœ¨å¾ˆå¤šå¿…é¡»äº†è§£çš„åŸºæœ¬ç»“æ„ï¼Œå› æ­¤æœ¬èŠ‚é¦–å…ˆä»‹ç» Flow
çš„åŸºç¡€æ¡†æ¶ï¼Œç„¶åè¯¦ç»†è¯´æ˜ NICE ã€Real NVP å’Œ Glow ç­‰å¸¸è§„æµã€i-ResNet ä»¥åŠå˜åˆ†æµç­‰æ¨¡å‹çš„ç»“
æ„ã€‚



## 5.1. æµæ¨¡å‹æ¡†æ¶

æ•°æ®åˆ†å¸ƒ $P(x)$ é€šè¿‡è½¬æ¢å‡½æ•° $F(x)$ å°†è¯¥åˆ†å¸ƒæ˜ å°„ä¸ºæŒ‡å®šçš„**ç®€å•åˆ†å¸ƒ**ï¼Œ**å‡è®¾**è¯¥åˆ†å¸ƒæ˜¯**å„åˆ†é‡ç‹¬ç«‹**çš„**é«˜æ–¯åˆ†å¸ƒ**ï¼Œåˆ™ $P(x)$ å¯ä»¥è¡¨ç¤ºæˆå¸¦æœ‰è½¬æ¢å‡½æ•°å’Œé›…å¯æ¯”è¡Œåˆ—å¼çš„å¦‚ä¸‹å½¢å¼ï¼š
$$
\begin{equation}
 P(x)=\frac{1}{(2 \pi)^{D / 2}} \exp \left(-\frac{1}{2}\|F(x)\|^{2}\right) \mid \operatorname{det}\left[\frac{\partial F}{\partial x}\right] 
\end{equation}
$$
å…¶ä¸­ $det(\cdot)$ è¡¨ç¤ºé›…å¯æ¯”è¡Œåˆ—å¼ã€‚æ ¹æ®è¯¥ç›®æ ‡å‡½æ•°ä¼˜åŒ–èƒ½å¾—åˆ° $F(x)$ ä¸­çš„å‚æ•°ï¼Œè¿›è€Œå¾—çŸ¥é€†å‡½æ•° $G(z)$ çš„å…·ä½“å½¢å¼ï¼Œè¿™æ ·å°±èƒ½å¾—åˆ°ä¸€ä¸ªç”Ÿæˆæ¨¡å‹ã€‚ä½†é›…å¯æ¯”è¡Œåˆ—å¼çš„è®¡ç®—é‡å¾ˆå¤§ï¼Œè½¬æ¢å‡½æ•°çš„é€†å˜æ¢éš¾ä»¥æ±‚è§£ï¼Œä¸ºäº†ä¿è¯è®¡ç®—ä¸Šçš„å¯è¡Œæ€§ï¼Œ$F(x)$ å¿…é¡»æ»¡è¶³å¦‚ä¸‹æ¡ä»¶ï¼š

â€‹	â‘ é›…å¯æ¯”è¡Œåˆ—å¼å®¹æ˜“è®¡ç®—ï¼›
â€‹	â‘¡å‡½æ•°å¯é€†ï¼Œæ±‚é€†è¿‡ç¨‹çš„è®¡ç®—é‡å°½é‡å°ã€‚

é›…å¯æ¯”è¡Œåˆ—å¼çš„ç»´æ•°ä¸æ•°æ®ç»´æ•°ç›¸å…³ï¼Œå¯¹äº**é«˜ç»´æ•°æ®**è€Œè¨€ï¼Œ**é›…å¯æ¯”è¡Œåˆ—å¼çš„è®¡ç®—é‡è¦æ¯”å‡½æ•°æ±‚é€†æ›´å¤§**ï¼Œå› æ­¤ $F(x)$ é¦–å…ˆè¦æ»¡è¶³ç¬¬ä¸€ä¸ªæ¡ä»¶ã€‚æµæ¨¡å‹æå‡ºå°†é›…å¯æ¯”è¡Œåˆ—å¼è®¾è®¡ä¸ºå®¹æ˜“è®¡ç®—çš„**ä¸‰è§’é˜µè¡Œåˆ—å¼**ï¼Œå…¶å€¼ç­‰äºå¯¹è§’çº¿å…ƒç´ ä¹˜ç§¯ä»è€Œç®€åŒ–æ±‚è§£é›…å¯æ¯”è¡Œåˆ—å¼çš„è®¡ç®—é‡:
$$
\begin{equation}
 \left|\operatorname{det}\left[\frac{d h^{i}}{d h^{i-1}}\right]\right|=\operatorname{sum} \left|\operatorname{diag}\left[\frac{d h^{i}}{d h^{i-1}}\right] \right| 
\end{equation}
$$
ä¸‰è§’é˜µè¡Œåˆ—å¼çš„ä¸Šä¸‰è§’æˆ–ä¸‹ä¸‰è§’åŒºåŸŸå…ƒç´ çš„å€¼ä¸º 0 æ„å‘³ç€æ¯æ¬¡è½¬æ¢éƒ½**åªæœ‰ä¸€éƒ¨åˆ†å…ƒç´ å‚ä¸äº†æ˜ å°„**ï¼Œ**å¦ä¸€éƒ¨åˆ†å…ƒç´ åªè¿›è¡Œäº†æ’ç­‰å˜æ¢**ï¼Œè¿™ç§ç®€å•å˜æ¢**äº§ç”Ÿçš„éçº¿æ€§è¾ƒå¼±**ï¼Œéœ€è¦å¤šä¸ªç®€å•å˜æ¢çš„å¤åˆå½¢å¼å¢å¼ºæ¨¡å‹çš„æ‹Ÿåˆèƒ½åŠ›ã€‚æ ¹æ®é“¾å¼æ³•åˆ™å¯å¾—ï¼š
$$
\begin{equation}
 \frac{\partial z}{\partial x}=\frac{\partial h^{1}}{\partial x} \cdot \frac{\partial h^{2}}{\partial h^{1}} \cdots \frac{\partial h^{k}}{\partial h^{k-1}} \cdot \frac{\partial z}{\partial h^{k}} 
\end{equation}
$$
æµæ¨¡å‹çš„è½¬æ¢å‡½æ•°ç”¨ç¥ç»ç½‘ç»œè¡¨ç¤ºï¼Œè¯¥ç¥ç»ç½‘ç»œç›¸å½“äºä¸€ç³»åˆ—è½¬æ¢å‡½æ•°ä½œç”¨æ•ˆæœçš„ç´¯ç§¯ï¼Œè¿™ç§ç®€å•å˜æ¢çš„å åŠ è¿‡ç¨‹å¦‚åŒæµæ°´ä¸€èˆ¬ç§¯å°‘æˆå¤šï¼Œå› æ­¤å°†è¿™æ ·çš„è¿‡ç¨‹ç§°ä¸º â€˜æµâ€™ï¼Œå¤§éƒ¨åˆ†æµæ¨¡å‹éƒ½ä»¥è¿™ç§æ¨¡å‹æ¡†æ¶ä¸ºåŸºç¡€ã€‚æ­¤æ—¶æµæ¨¡å‹çš„å¯¹æ•°ä¼¼ç„¶å‡½æ•°å¯ä»¥å†™æˆï¼š
$$
\begin{equation}
 \begin{aligned} & \log P(x) \\=&-\log P(z)-\sum_{i=1}^{k} \log \left|\operatorname{det}\left(\frac{d h_{i}}{d h_{i-1}}\right)\right| \\=&-\sum_{i=1}^{k}\left(\frac{1}{2}\left\|F^{i}(x)\right\|^{2}-\log \left|\operatorname{det}\left(\frac{d h_{i}}{d h_{i-1}}\right)\right|\right)+c \end{aligned} 
\end{equation}
$$
å…¶ä¸­ $ c=-\frac{D}{2} \log (2 \pi) $ è¡¨ç¤ºå¸¸æ•°ã€‚







---

## 5.2 å¸¸è§„æµ

### 5.2.1 NICE

éçº¿æ€§ç‹¬ç«‹æˆåˆ†ä¼°è®¡ï¼ˆNonlinear Independent Components Estimationï¼ŒNICEï¼‰[5] æ˜¯ç¬¬ä¸€ä¸ªæµæ¨¡å‹ï¼Œæ­¤åå‡ºç°çš„æµæ¨¡å‹å¤§éƒ¨åˆ†éƒ½æ˜¯ä»¥ NICE çš„ç»“æ„å’Œç†è®ºä¸ºåŸºç¡€ã€‚é™¤äº†æµæ¨¡å‹çš„åŸºæœ¬æ¡†æ¶å¤–ï¼Œ NICE æå‡ºäº†ä¸‰ä¸ªé‡è¦çš„æ¨¡å‹ç»“æ„ï¼šåŠ æ€§è€¦åˆå±‚ã€ç»´æ•°æ··åˆå’Œç»´æ•°å‹ç¼©å±‚ã€‚



#### åŠ æ€§è€¦åˆå±‚

NICE æå‡ºå°†é›…å¯æ¯”è¡Œåˆ—å¼æ„é€ æˆä¸‰è§’é˜µå½¢å¼ï¼Œå¹¶å°†è¿™ç§ç»“æ„ç§°ä¸ºè€¦åˆå±‚ï¼ˆ coupling layer ï¼‰ã€‚è€¦åˆå±‚å°† $D$ ç»´è¾“å…¥å˜é‡åˆ†å‰²æˆä¸¤éƒ¨åˆ† $ x_{D}=\left[x_{1: d}, x_{d+1, D}\right]=\left[x_{1}, x_{2}\right] $ï¼Œç„¶åå–å¦‚ä¸‹å˜æ¢ï¼š
$$
\begin{equation}
 h_{1}=x_{1} \\ h_{2}=x_{2}+M\left(x_{1}\right) 
\end{equation}
$$
å…¶ä¸­ $M$ è¡¨ç¤ºå®šä¹‰åœ¨ç©ºé—´ $ \mathbf{R}^{d} $ ä¸Šçš„ä»»æ„å‡½æ•°ï¼Œä¸‹ä¸€ä¸ªéšè—å±‚å˜é‡ä¸º $ h=\left[h_{1}, h_{2}\right] $ï¼Œè¿™ç§åªå«æœ‰åŠ æ€§ç®—æ³•çš„è€¦åˆå±‚è¢«ç§°ä¸ºåŠ æ€§è€¦åˆå±‚ï¼ˆ Additive Coupling ï¼‰ï¼Œå…¶ç»“æ„å¦‚ä¸‹ **Fig. 15** æ‰€ç¤ºã€‚

![1627374646445](assets/1627374646445.png)

**Fig. 15** åŠ æ€§è€¦åˆå±‚ç»“æ„

åŠ æ€§è€¦åˆå±‚çš„é›…å¯æ¯”è¡Œåˆ—å¼æ˜¯ä¸Šä¸‰è§’è¡Œåˆ—å¼ä¸”å¯¹è§’çº¿å…ƒç´ å…¨éƒ¨ä¸º 1 ï¼Œç”¨åˆ†å—çŸ©é˜µè¡¨ç¤ºè¯¥è¡Œåˆ—å¼ä¸ºï¼š
$$
\begin{equation}
 \frac{\partial h}{\partial x}=\left[\begin{array}{cc}\partial h_{1} / \partial x_{1} & \partial h_{1} / \partial x_{2} \\ \partial h_{2} / \partial x_{1} & \partial h_{2} / \partial x_{2}\end{array}\right]=\left[\begin{array}{cc}I_{d} & 0 \\ \partial h_{2} / \partial x_{1} & I_{D-d}\end{array}\right]=1 
\end{equation}
$$
è¯¥é›…å¯æ¯”è¡Œåˆ—å¼çš„å€¼ä¸º 1 ï¼Œæ ¹æ®é“¾å¼æ³•åˆ™å¯ä»¥å¾—åˆ°ï¼š
$$
\begin{equation}
 \operatorname{det}\left[\frac{\partial z}{\partial x}\right]=\operatorname{det}\left[\frac{\partial h^{1}}{\partial x}\right] \cdots \operatorname{det}\left[\frac{\partial z}{\partial h^{k}}\right]=1 
\end{equation}
$$
è¿™ä½¿å¾—è¯¥é¡¹åœ¨ç›®æ ‡å‡½æ•°ä¸­çš„å€¼ä¸º 1 ï¼Œä»è€Œæ¶ˆé™¤äº†é›…å¯æ¯”è¡Œåˆ—å¼çš„è®¡ç®—é‡ã€‚è¯¥è½¬æ¢å‡½æ•°çš„é€†å‡½æ•°ä¹Ÿå¾ˆå®¹æ˜“å¾—åˆ°ï¼Œå…¶é€†å˜æ¢çš„å½¢å¼å¦‚ä¸‹ï¼š
$$
\begin{equation}
 x_{1}=h_{1} \\
 x_{2}=h_{2}-M\left(h_{1}\right) 
\end{equation}
$$
è¿™ç§ç»“æ„çš„è½¬æ¢å‡½æ•°å³æ»¡è¶³å¯é€†æ€§çš„è¦æ±‚ï¼Œä¸”é€†å‡½æ•°å’Œé›…å¯æ¯”è¡Œåˆ—å¼éƒ½å®¹æ˜“æ±‚è§£ï¼Œä¸éœ€è¦é¢å¤–çš„è®¡ç®—é‡ï¼Œåæ¥å¤§éƒ¨åˆ†çš„æµæ¨¡å‹éƒ½é‡‡ç”¨äº†è¿™ç§ç»“æ„ã€‚



---

#### ç»´æ•°æ··åˆ

è½¬æ¢å‡½æ•°ä¸ä»…éçº¿æ€§èƒ½åŠ›è¾ƒå¼±ï¼Œè€Œä¸”æ¯æ¬¡è½¬æ¢è¿‡ç¨‹éƒ½æœ‰ä¸€éƒ¨åˆ†å…ƒç´ æ²¡æœ‰å˜åŒ–ã€‚ä¸ºäº†ä½¿ä¿¡æ¯èƒ½å……åˆ†æ··åˆï¼Œ NICE é‡‡ç”¨åœ¨æ¯æ¬¡è€¦åˆå±‚åç›´æ¥äº¤æ¢ä¸¤éƒ¨åˆ†å…ƒç´ çš„ä½ç½® $ h_{1}^{1}=h_{2}^{2}, \quad h_{2}^{1}=h_{1}^{2} $ï¼Œå…¶ç»“æ„å¦‚å›¾ 16 æ‰€ç¤ºã€‚

![1627374745608](assets/1627374745608.png)

**Fig. 16** ç»´æ•°æ··åˆç»“æ„



---

#### ç»´æ•°å‹ç¼©å±‚

Flow æ˜¯ä»¥å¯é€†å˜æ¢ç»“æ„ä¸ºåŸºç¡€çš„æ¨¡å‹ï¼Œå˜æ¢å¯é€†æ€§ä½¿å¾—æ¨¡å‹ä¸­**å„éšè—å±‚çš„ç»´æ•°**éœ€è¦ä¸**è¾“å…¥æ ·æœ¬ç»´æ•° $D$ çš„å¤§å°ç›¸åŒ**ï¼Œè¿™ä½¿å¾— Flow æ¨¡å‹å­˜åœ¨ä¸¥é‡çš„ç»´æ•°æµªè´¹é—®é¢˜ï¼Œå› æ­¤ NICE æå‡ºåœ¨**æœ€åä¸€å±‚å’Œå…ˆéªŒåˆ†å¸ƒä¹‹é—´å¼•å…¥ç»´æ•°å‹ç¼©å±‚**ï¼Œæ­¤æ—¶æ¨¡å‹çš„å¯¹æ•°ä¼¼ç„¶å‡½æ•°å˜ä¸º
$$
\begin{equation}
 \begin{aligned} \log P(x)=&-\frac{D}{2} \log (2 \pi)-\sum_{i=1}^{k}\left(\frac{1}{2}\left\|G^{i}(x)\right\|^{2}\right) \\ &-\frac{1}{2}\|s \cdot G(x)\|^{2}+\sum_{i=1}^{D} \log s_{i} \end{aligned} 
\end{equation}
$$
å…¶ä¸­ $s$ è¡¨ç¤ºç»´æ•°å‹ç¼©å±‚ä¸­å¾…ä¼˜åŒ–çš„å‚æ•°ã€‚åœ¨å‹ç¼©å±‚ä¸­å¼•å…¥ $s$ ç­‰ä»·äºå°†å…ˆéªŒåˆ†å¸ƒçš„æ–¹å·®ä¹Ÿä½œä¸ºå‚æ•°è¿›è¡Œä¼˜åŒ–ã€‚å¦‚æœæŸä¸ªæ–¹å·®æ¥è¿‘ 0 ï¼Œè¯´æ˜å…¶å¯¹åº”çš„ç»´æ•°æ‰€è¡¨ç¤ºçš„æµå½¢å·²ç»å¡Œç¼©ä¸ºç‚¹ï¼Œä»è€Œèµ·åˆ°ç»´æ•°å‹ç¼©çš„ä½œç”¨ã€‚



---

### 5.2.2 RealNVP

Real NVP [63] çš„å…¨ç§°ä¸º real-valued non-volume preserving ï¼Œç›´è¯‘ä¸ºå®å€¼éä½“ç§¯ä¿æŒï¼Œéä½“ç§¯ä¿æŒæ˜¯æŒ‡è¯¥æ¨¡å‹çš„é›…å¯æ¯”è¡Œåˆ—å¼çš„å€¼ä¸ä¸º 1 ã€‚ Real NVP åœ¨NICE çš„åŸºæœ¬ç»“æ„ä¸Šï¼Œæå‡ºäº†æ¯”åŠ æ€§è€¦åˆå±‚**éçº¿æ€§èƒ½åŠ›æ›´å¼º**çš„**ä»¿å°„è€¦åˆå±‚å’Œç»´æ•°çš„éšæœºæ‰“ä¹±æœºåˆ¶**ï¼Œåœ¨**è€¦åˆå±‚ä¸­å¼•å…¥å·ç§¯å±‚**ä½¿å¾— Flow æ¨¡å‹å¯ä»¥æ›´å¥½åœ°å¤„ç†å›¾åƒé—®é¢˜ï¼Œå¹¶è®¾è®¡äº†å¤šå°ºåº¦ç»“æ„ä»¥**é™ä½ NICE æ¨¡å‹çš„è®¡ç®—é‡å’Œå­˜å‚¨ç©ºé—´**ã€‚



#### ä»¿å°„è€¦åˆå±‚ 

NICE æ€§èƒ½è¾ƒå·®ä¸è€¦åˆå±‚ç»“æ„è¿‡äºç®€å•æœ‰å…³ï¼Œå› æ­¤ Real NVP æå‡ºåœ¨åŸæœ‰çš„åŠ æ€§è€¦åˆå±‚çš„åŸºç¡€ä¸ŠåŠ å…¥äº†ä¹˜æ€§è€¦åˆï¼Œä¸¤è€…ç»„æˆçš„æ··åˆå±‚ç§°ä¸ºä»¿å°„è€¦åˆå±‚ï¼ˆ affine coupling layer ï¼‰ï¼Œå…¶ç»“æ„å¦‚å›¾ 17 æ‰€ç¤ºã€‚

![1627374954487](assets/1627374954487.png)

**Fig. 17** ä»¿å°„è€¦åˆå±‚ç»“æ„

è¯¥è€¦åˆå±‚å¯ä»¥è¡¨ç¤ºæˆå¦‚ä¸‹å½¢å¼ï¼š
$$
\begin{equation}
 h_{1}=x_{1} \\
 h_{2}=x_{2} \odot M_{2}\left(x_{1}\right)+M_{1}\left(x_{1}\right) 
\end{equation}
$$
ä»¿å°„è€¦åˆå±‚çš„é›…å¯æ¯”è¡Œåˆ—å¼æ˜¯å¯¹è§’çº¿ä¸å…¨ä¸º 1 çš„ä¸‹ä¸‰è§’é˜µï¼Œç”¨åˆ†å—çŸ©é˜µè¡¨ç¤ºè¯¥è¡Œåˆ—å¼ä¸ºï¼š
$$
\begin{equation}
 \frac{\partial h}{\partial x}=\left[\begin{array}{cc}I_{d} & 0 \\ \frac{\partial h_{2}}{\partial x_{1}} & M_{2}\left(x_{1}\right)\end{array}\right] 
\end{equation}
$$
è¯¥è¡Œåˆ—å¼çš„å€¼ä¸ºå¯¹è§’çº¿å…ƒç´ ä¹˜ç§¯ ï¼Œä¸ºäº†ä¿è¯**å¯é€†æ€§**éœ€è¦çº¦æŸé›…å¯æ¯”è¡Œåˆ—å¼**å¯¹è§’çº¿å„å…ƒç´ å‡å¤§äº**
**0**ï¼Œå› æ­¤ Real NVP **ç›´æ¥ç”¨ç¥ç»ç½‘ç»œè¾“å‡º $ \log s $** ã€‚ è¯¥è½¬æ¢å‡½æ•°çš„é€†å‡½æ•°å¾ˆå®¹æ˜“è¡¨ç¤ºä¸ºï¼š
$$
\begin{equation}
 x_{1}=h_{1} \\
 x_{2}=\frac{h_{2}-M_{1}\left(x_{1}\right)}{M_{2}\left(x_{1}\right)} 
\end{equation}
$$

---

#### éšæœºæ··åˆæœºåˆ¶ 

NICE æ€§èƒ½è¾ƒå·®çš„å¦ä¸€ä¸ªåŸå› æ˜¯äº¤æ¢ä¸¤ä¸ªåˆ†é‡çš„ä½ç½®ä¸èƒ½å……åˆ†æ··åˆå˜é‡ä¿¡æ¯ï¼Œå› æ­¤ Real NVP é‡‡ç”¨éšæœºæ··åˆæœºåˆ¶ï¼Œå¯¹è€¦åˆå±‚ä¹‹é—´çš„åˆ†é‡éšæœºæ‰“ä¹±ï¼Œå†å°†æ‰“ä¹±åçš„å‘é‡é‡æ–°åˆ†å‰²æˆä¸¤éƒ¨åˆ†å¹¶è¾“é€åˆ°ä¸‹ä¸ªè€¦åˆå±‚ä¸­ï¼Œå…¶ç»“æ„å¦‚å›¾ 18 æ‰€ç¤ºã€‚

![1627375103521](assets/1627375103521.png)

**Fig. 18** éšæœºæ··åˆç»“æ„



---

#### æ©ç å·ç§¯å±‚ 

ä¸ºäº†æ›´å¥½çš„å¤„ç†å›¾ç‰‡æ ·æœ¬ï¼Œ RealNVP åœ¨æµæ¨¡å‹ä¸­å¼•å…¥äº†å·ç§¯å±‚ã€‚å·ç§¯æ–¹æ³•å¯ä»¥æ•æ‰æ ·æœ¬åœ¨ç©ºé—´ä¸Šçš„å±€éƒ¨ç›¸å…³æ€§ï¼Œä½†æ˜¯**éšæœºæ‰“ä¹±æœºåˆ¶ä¼šä½¿æ ·æœ¬åŸæœ‰çš„å±€éƒ¨ç›¸å…³æ€§æ¶ˆå¤±**ï¼Œä¸ºæ­¤ Real NVP æå‡ºå…ˆä½¿ç”¨æ©ç **å¢åŠ æ ·æœ¬é€šé“æ•°**å¹¶**é™ä½ç©ºé—´ç»´æ•°**ï¼Œæ£‹ç›˜æ©ç æ˜¯ä¸€ç§å›ºå®šé—´éš”çš„ç©ºé—´è½´ä¸Šçš„äº¤é”™æ©ç ï¼Œèƒ½å¤Ÿæœ‰æ•ˆä¿ç•™æ ·æœ¬åœ¨ç©ºé—´çš„å±€éƒ¨ç›¸å…³æ€§ï¼š
$$
\begin{equation}
 h \times w \times c \rightarrow \frac{1}{n} h \times \frac{1}{n} w \times 2 n c 
\end{equation}
$$
ç”¨æ£‹ç›˜æ©ç å¢åŠ æ ·æœ¬é€šé“æ•°çš„æ“ä½œç§°ä¸ºæŒ¤å‹ï¼ˆ squeeze ï¼‰ï¼Œæ˜¯æµæ¨¡å‹ä¸­ä½¿ç”¨å·ç§¯å±‚çš„å¿…é¡»æ­¥éª¤ï¼Œç„¶åå¯¹æ ·æœ¬çš„é€šé“æ‰§è¡Œåˆ†å‰²å’Œæ‰“ä¹±æ“ä½œï¼Œè¿™ç§æ–¹å¼**ä¿ç•™äº†æ ·æœ¬çš„å±€éƒ¨ç›¸å…³æ€§**ï¼Œä»¥ä¾¿ç›´æ¥ä½¿ç”¨å·ç§¯ç½‘ç»œï¼Œå¤§å¹…åº¦**æé«˜æ¨¡å‹çš„è®¡ç®—æ•ˆç‡**ã€‚



---

#### å¤šå°ºåº¦ç»“æ„ 

NICE çš„åŠ æ€§è€¦åˆå±‚å’Œ real NVPçš„ä»¿å°„è€¦åˆå±‚åœ¨æ¯æ¬¡æ‰§è¡Œæ—¶éƒ½æœ‰éƒ¨åˆ†ç»´æ•°çš„å‘é‡æ²¡æœ‰æ”¹å˜ï¼Œå› æ­¤ real NVP æå‡ºåœ¨ä»¿å°„è€¦åˆå±‚ä¸­ä½¿ç”¨å¦‚ Fig. 19 æ‰€ç¤ºçš„å¤šå°ºåº¦ç»“æ„ï¼Œæ˜¯ä»¿å°„è€¦åˆå±‚äº¤æ›¿å˜æ¢çš„ä¸€ç§ç»„åˆç»“æ„ã€‚

å°†æ ·æœ¬åˆ†æˆå››éƒ¨åˆ† $ x=\left[x_{1}, x_{2}, x_{3}, x_{4}\right] $ è¾“å…¥åˆ°è€¦åˆå±‚ä¸­ï¼Œç¬¬ä¸€æ¬¡è½¬æ¢å°† $x_1$ å’Œ $x_2$ è½¬æ¢æˆ $h_1$ å’Œ
$h_2$ åå½“ä½œå¤šå°ºåº¦ç»“æ„çš„ç»“æœ $z_1$ å’Œ $z_2$ï¼Œç„¶åå°†æ²¡æœ‰æ”¹å˜çš„ $h_3^1$ å’Œ $h_4^1$ è¾“å…¥åˆ°è€¦åˆå±‚ä¸­ç»§ç»­è½¬æ¢ï¼Œå¾—åˆ°è½¬æ¢åçš„ç»“æœ $z_3$ å’Œæ²¡æœ‰æ”¹å˜çš„ $h_4^2$ï¼Œæœ€ååœ¨ç¬¬ä¸‰æ¬¡è½¬æ¢è¿‡ç¨‹ä¸­å°† $h_4^2$ è½¬æ¢æˆ $z_4$ ã€‚

#### 

![1627375231523](assets/1627375231523.png)

**Fig. 19** ä»¿å°„è€¦åˆå±‚çš„ç»„åˆç­–ç•¥

å¤šå°ºåº¦ç»“æ„é€šè¿‡è¿™ç§é€å±‚è½¬æ¢çš„æ–¹å¼ï¼Œä½¿æ•°æ®çš„å…¨éƒ¨å…ƒç´ éƒ½å¯ä»¥åœ¨ä¸€ä¸ªå¤åˆè€¦åˆå±‚å†…è¿›è¡Œè½¬æ¢ï¼Œä¿ç•™äº†åŸæœ‰æ–¹æ³•ä¸­é›…å¯æ¯”è¡Œåˆ—å¼å®¹æ˜“è®¡ç®—çš„ç‰¹ç‚¹ï¼Œå‡å°‘æ¨¡å‹å¤æ‚åº¦å’Œè®¡ç®—é‡çš„åŒæ—¶å¢åŠ æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ã€‚



---

### 5.2.3. GLOW

GLOW [64] æ˜¯ä»¥ NICE å’Œ RealNVP ä¸ºåŸºç¡€ç»“æ„çš„æ¨¡å‹ï¼Œæ˜¯å½“å‰æµæ¨¡å‹ä¸­æ•ˆæœæœ€å¥½çš„æ¨¡å‹ã€‚ GLOW æ¨¡å‹ä¸»è¦æœ‰ä¸¤ä¸ªè´¡çŒ®ï¼šç¬¬ä¸€ä¸ªè´¡çŒ®æ˜¯**ä¿®æ”¹æµæ¨¡å‹çš„ç»“æ„**ï¼Œæå‡ºå®Œæ•´çš„æ¨¡å‹ç»“æ„ï¼Œå¼•å…¥ Actnorm å±‚ï¼›ç¬¬äºŒä¸ªè´¡çŒ®æ˜¯æå‡º **1x1 å·ç§¯**å’Œ **LU çŸ©é˜µåˆ†è§£**æ–¹æ³•å¹¶å°†ç½®æ¢çŸ©é˜µå½“ä½œä¼˜åŒ–é¡¹ã€‚

#### æ¨¡å‹ç»“æ„ä¿®æ”¹ 

GLOW ä»¥ RealNVP æ¨¡å‹ä¸ºåŸºç¡€æ„é€ äº†æ€§èƒ½æ›´å¥½çš„æ¨¡å‹æ¡†æ¶ï¼Œå¹¶é’ˆå¯¹ RealNVP çš„ä¸è¶³è¿›è¡Œä¸¤å¤„ä¿®æ”¹ï¼š

1. ä»¿å°„è€¦åˆå±‚å†…éƒ¨çš„ä¹˜æ€§è€¦åˆä½¿å¾—å…¶è®¡ç®—é‡æ˜¯åŠ æ€§è€¦åˆå±‚çš„ä¸¤å€ï¼Œä½†ç»è¿‡å®éªŒè¯æ˜ä»¿å°„è€¦åˆå±‚çš„
æ€§èƒ½æå‡å¾ˆå°ï¼Œå› æ­¤ GLOW è®­ç»ƒé«˜ç»´æ ·æœ¬æ—¶ä¸ºäº†å‡å°‘è®¡ç®—é‡**åªä¿ç•™åŠ æ€§è€¦åˆå±‚**ã€‚

2. GLOW è¯æ˜äº†**æ£‹ç›˜æ©ç çš„å¤æ‚æ“ä½œä¸èƒ½æå‡æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›**ï¼Œå› æ­¤**åˆ é™¤**äº†è¯¥æ¨¡å—ã€‚



#### Actnorm å±‚ 

ç”±äºå†…å­˜é™åˆ¶ï¼Œæµæ¨¡å‹åœ¨è®­ç»ƒè¾ƒå¤§çš„å›¾åƒæ—¶æ¯ä¸ªæ‰¹æ¬¡çš„æ ·æœ¬æ•°é€šå¸¸é€‰ 1 ï¼Œå› æ­¤æå‡ºäº†ç±»ä¼¼äºæ‰¹å½’ä¸€åŒ–å¤„ç†çš„ Actnorm å±‚ã€‚ Actnorm ç”¨**æ‰¹æ¬¡æ ·æœ¬çš„å‡å€¼å’Œæ–¹å·®åˆå§‹åŒ–å‚æ•°** $b$ å’Œ $s$ ï¼Œæ˜¯**å¯¹å…ˆéªŒåˆ†å¸ƒçš„å¹³ç§»å’Œç¼©æ”¾ï¼Œæœ‰åŠ©äºæé«˜æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›**ã€‚



#### ç½®æ¢çŸ©é˜µ 

ç›¸æ¯”äº NICE ä¸­çš„ç®€å•äº¤æ¢ï¼ŒRealNVP çš„éšæœºæ‰“ä¹±æ–¹æ³•å¯ä»¥å¾—åˆ°æ›´ä½çš„æŸå¤±ï¼Œå› æ­¤ GLOW æå‡ºç”¨ **1x1 å·ç§¯**è¿ç®—æ”¹å˜ç½®æ¢é€šé“çš„æ’åˆ—ï¼Œç”¨**ç½®æ¢çŸ©é˜µæ›¿ä»£éšæœºæ‰“ä¹±**å¹¶æ”¾åˆ°æŸå¤±å‡½æ•°ä¸­ä¸€å¹¶ä¼˜åŒ–ä»¥è¿›ä¸€æ­¥æå‡æ¨¡å‹æ•ˆæœã€‚

å…·ä½“æ–¹æ³•æ˜¯é€šè¿‡ä¸€ä¸ªéšæœºæ—‹è½¬çŸ©é˜µ $W$ ç½®æ¢è¾“å…¥è½´é€šé“çš„æ’åˆ—é¡ºåºä½¿ $ h=x W $ï¼Œä¸ºäº†ä¿è¯è½¬æ¢å‡½æ•°çš„å¯é€†æ€§ï¼Œæ–¹é˜µ $W$ åˆå§‹åŒ–ä¸º**éšæœºæ­£äº¤çŸ©é˜µ**ï¼Œå› æ­¤å…¶é›…å¯æ¯”è¡Œåˆ—å¼çš„å€¼ä¸º $ \operatorname{det} W $ã€‚

ä¸ºäº†æ›´å®¹æ˜“è®¡ç®—é›…å¯æ¯”è¡Œåˆ—å¼çš„å€¼ï¼Œ GLOW åˆ©ç”¨ **LU çŸ©é˜µåˆ†è§£æ³•**åˆ†è§£æ­£äº¤çŸ©é˜µ $W$ ä½¿ $ W=P L U $ï¼Œå…¶ä¸­ $P$ æ˜¯ç½®æ¢çŸ©é˜µï¼Œ$L$ æ˜¯å¯¹è§’çº¿å…¨ä¸º 1 çš„ä¸‹ä¸‰è§’é˜µï¼Œ$U$ æ˜¯ä¸Šä¸‰è§’é˜µï¼Œæ­¤æ—¶å¯ä»¥å®¹æ˜“å¾—åˆ°é›…å¯æ¯”è¡Œåˆ—å¼çš„å€¼ä¸ºä¸Šä¸‰è§’é˜µ $U$ çš„å¯¹è§’çº¿ä¹˜ç§¯ï¼š
$$
\begin{equation}
 \log |\operatorname{det} W|=\sum \log |\operatorname{diag}(U)| 
\end{equation}
$$
GLOW ä½¿ç”¨ LU åˆ†è§£æ³•è®¡ç®—æ—‹è½¬çŸ©é˜µ $W$ çš„é›…å…‹æ¯”è¡Œåˆ—å¼çš„å€¼ï¼Œå‡ ä¹æ²¡æœ‰æ”¹å˜åŸæ¨¡å‹çš„è®¡ç®—é‡ï¼Œä¸”å‡å°‘äº†å¾…ä¼˜åŒ–å‚æ•°çš„æ•°é‡ã€‚å®éªŒè¯æ˜äº†å¯é€† 1x1 å·ç§¯å¯ä»¥å¾—åˆ°æ¯”éšæœºæ‰“ä¹±æœºåˆ¶**æ›´ä½çš„æŸå¤±**ä¸”å…·æœ‰**å¾ˆå¥½çš„ç¨³å®šæ€§**ã€‚

![1627375726737](assets/1627375726737.png)

**Fig. 20** GLOW çš„å±‚ç»“æ„

GLOW çš„å•ä¸ªè½¬æ¢ç»“æ„åŒ…æ‹¬ **Actnorm å±‚**ã€**å¯é€† 1x1 å·ç§¯**å’Œ**è€¦åˆå±‚**ï¼Œå…¶æµç¨‹å›¾å¦‚ Fig. 20 æ‰€ç¤ºã€‚å›¾ä¸­çš„è¶…å‚æ•° $K$ å’Œ $L$ è¡¨ç¤ºå¾ªç¯æ¬¡æ•°ã€‚æ ·æœ¬ $x$ å…ˆè¿›è¡Œ squeeze æ“ä½œåç”¨å•æ­¥è½¬æ¢ç»“æ„è¿­ä»£ $K$ æ¬¡ï¼Œç„¶åå°†è½¬æ¢çš„ç»“æœè¿›è¡Œç»´æ•°åˆ†å‰²ï¼Œåˆ†å‰²åçš„ä¸¤éƒ¨åˆ†å˜é‡ä¸å¤šå°ºåº¦ç»“æ„çš„ç»“æœæ„ä¹‰ç›¸åŒï¼Œå°†æ•´ä¸ªè¿‡ç¨‹å¾ªç¯ $L-1$ æ¬¡åå°†æœªè½¬æ¢è¿‡çš„éƒ¨åˆ†ç»´æ•°å†æ¬¡è¿›è¡Œ squeeze æ“ä½œå’Œ $K$ æ¬¡å•æ­¥è½¬æ¢ï¼Œä»¥ä¸Šæ„æˆäº† GLOW çš„å¤šå°ºåº¦ç»“æ„ã€‚

GLOW è¿›ä¸€æ­¥æå‡äº†æµæ¨¡å‹çš„æ€§èƒ½ï¼Œå„ä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°éƒ½è¶…è¿‡äº†å…¶ä»–æ‰€æœ‰æµæ¨¡å‹ï¼Œå¯ä»¥ç”Ÿæˆæ¸…æ™°åº¦å¾ˆé«˜çš„äººè„¸å›¾åƒï¼Œç¼ºç‚¹æ˜¯ç½®æ¢çŸ©é˜µå¯¼è‡´æ¨¡å‹çš„å±‚æ•°å¾ˆå¤šï¼Œæ‹¥æœ‰ç”Ÿæˆå¼æ¨¡å‹ä¸­æœ€å¤§çš„å‚æ•°é‡çº§ï¼Œä¾‹å¦‚ç”Ÿæˆ 256x256 çš„é«˜æ¸…äººè„¸å›¾åƒéœ€è¦ 600 å¤šä¸ªè€¦åˆå±‚å’Œ 2 äº¿å¤šä¸ªå‚æ•°ï¼Œè®­ç»ƒæˆæœ¬å¾ˆé«˜ï¼Œå› æ­¤æ”¹è¿›è‡ªèº«ç»“æ„æˆ–ä½¿ç”¨éçº¿æ€§ç¨‹åº¦æ›´é«˜çš„è½¬æ¢å‡½æ•°ä»¥é™ä½è®­ç»ƒæˆæœ¬å’Œæ¨¡å‹æ·±åº¦æ˜¯æé«˜æµæ¨¡å‹å®ç”¨æ€§çš„å…³é”®ã€‚





---

## 5.3 å¯é€†æ®‹å·®ç½‘ç»œ

ä»¥ GLOW ä¸ºä»£è¡¨çš„å¸¸è§„æµæ¨¡å‹æœ‰ä¸¤ä¸ªä¸¥é‡çš„é—®é¢˜ï¼šç¬¬ä¸€ä¸ªé—®é¢˜æ˜¯æµæ¨¡å‹**ä¸ºäº†ä¿è¯è½¬æ¢å‡½æ•°çš„é›…**
**å¯æ¯”è¡Œåˆ—å¼åœ¨è®¡ç®—é‡ä¸Šçš„å¯è¡Œæ€§**ï¼Œå¯¼è‡´**å•å±‚è½¬æ¢å‡½æ•°çš„éçº¿æ€§å˜æ¢èƒ½åŠ›å¾ˆå¼±**ï¼Œè¿‡å¤šè€¦åˆå±‚çš„ç´¯åŠ ä½¿æ¨¡å‹çš„å‚æ•°ä¸ªæ•°å·¨å¤§ï¼›ç¬¬äºŒä¸ªé—®é¢˜æ˜¯ä¸ºäº†æœ‰ä¸€ä¸ªå®¹æ˜“æ±‚è§£çš„é€†å‡½æ•°ï¼Œ**æµæ¨¡å‹çš„è€¦åˆå±‚çš„å­˜åœ¨ï¼Œå¯¼è‡´æ¨¡å‹æ˜¯ä¸å¯¹ç§°çš„**ã€‚

å¯é€†æ®‹å·®ç½‘ç»œï¼ˆ Invertible Residual Networks,i-ResNet ï¼‰æ˜¯**ä»¥æ®‹å·®ç½‘ç»œä¸ºåŸºç¡€çš„ç”Ÿæˆæ¨¡å‹**ï¼Œ**åˆ©ç”¨çº¦æŸä½¿æ®‹å·®å—å¯é€†**ï¼Œç„¶åç”¨**è¿‘ä¼¼æ–¹æ³•è®¡ç®—**æ®‹å·®å—çš„**é›…å¯æ¯”è¡Œåˆ—å¼**ï¼Œè¿™ä½¿å¾— i-ResNet ä¸å…¶å®ƒæµæ¨¡å‹æœ‰æœ¬è´¨åŒºåˆ«ï¼š**ä¿ç•™äº† ResNet çš„åŸºæœ¬ç»“æ„å’Œæ‹Ÿåˆèƒ½åŠ›**ï¼Œä½¿æ®‹å·®å—æ˜¯å¯¹ç§°çš„åˆæœ‰å¾ˆå¼ºçš„éçº¿æ€§è½¬æ¢èƒ½åŠ›ã€‚



---

### 5.3.1 æ®‹å·®å—çš„å¯é€†æ€§æ¡ä»¶

i-ResNet çš„åŸºæœ¬æ¨¡å—ä¸ ResNet ç›¸åŒï¼Œå¯ä»¥è¡¨ç¤ºæˆ $ y=x+G(x) $ï¼Œæ®‹å·®å—ç”¨ç¥ç»ç½‘ç»œ $ x+G(x) $ æ‹Ÿåˆy ï¼Œä½¿å¾—æ®‹å·®å—çš„æ¢¯åº¦ $ 1+\partial G(x) / \partial y $ ä¸ä¼šåœ¨æ·±å±‚ç½‘ç»œä¸­å‡ºç°æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜ï¼Œä»¥ä¾¿è®­ç»ƒæ›´æ·±å±‚æ¬¡çš„ç½‘ç»œã€‚å°† i-ResNet æ„é€ æˆæµæ¨¡å‹ï¼Œé¦–å…ˆè¦ä¿è¯æ¨¡å‹çš„å¯é€†æ€§ï¼Œç­‰åŒäºä¿è¯å•ä¸ªæ®‹å·®å—çš„å¯é€†æ€§ã€‚æ®‹å·®å—å¯é€†æ€§çš„å……åˆ†ä¸å¿…è¦æ¡ä»¶æ˜¯å‡½æ•° $ G(\cdot) $ çš„ Lipschitz èŒƒæ•°å°äº 1 å³ $ \operatorname{Lip}(G)<1 $ã€‚å› æ­¤ç¥ç»ç½‘ç»œæ‹Ÿåˆçš„å‡½æ•° $ G(\cdot)=F(W x+b) $ ä½¿ç”¨æ™®é€šæ¿€æ´»å‡½æ•°æ—¶ï¼Œå…¶å¯é€†æ€§æ¡ä»¶ç­‰ä»·äºæƒé‡çŸ©é˜µ $W$ çš„è°±èŒƒæ•°å°äº 1 ï¼š
$$
\begin{equation}
 \operatorname{Lip}(G)<1 \Leftrightarrow \operatorname{Lip}(W)<1 
\end{equation}
$$
å› æ­¤åªè¦å¯¹ $ G(\cdot) $ å†…çš„æ‰€æœ‰æƒé‡çŸ©é˜µè¿›è¡Œè°±å½’ä¸€åŒ–åä¹˜ä¸€ä¸ªä»‹äº 0 å’Œ 1 ä¹‹é—´çš„ç³»æ•°å³å¯ä¿è¯æ®‹å·®
å—çš„å¯é€†æ€§ï¼š
$$
\begin{equation}
 W \leftarrow \frac{c W}{\|W\|_{2}} 
\end{equation}
$$

---

### 5.3.2 i-ResNet çš„æ±‚è§£æ–¹æ³•

æµæ¨¡å‹éœ€è¦ç›´æ¥è®¡ç®—å‡ºæ®‹å·®å—çš„é€†å‡½æ•°ï¼Œä½†æ®‹å·®å—çš„å½¢å¼å¯¼è‡´å¾ˆéš¾ç›´æ¥æ±‚å‡ºé€†å‡½æ•°çš„è§£æå½¢å¼ï¼Œä¸ºäº†ç®€åŒ–è®¡ç®—ï¼Œi-ResNet ä½¿ç”¨è¿­ä»£ $ x_{n+1}=y-G\left(x_{n}\right) $ï¼šå½“ $x_{n}$ æ”¶æ•›åˆ°æŸä¸ªå›ºå®šå‡½æ•°æ—¶è¡¨æ˜å¾—åˆ°äº†è¶³å¤Ÿè¿‘ä¼¼çš„é€†å‡½æ•°ï¼Œå¹¶ç»™å‡ºé™åˆ¶ $ \operatorname{Lip}(G)>0.5 $ ä¿è¯ $x_{n}$ çš„æ”¶æ•›æ€§ã€‚

i-ResNet çš„å…³é”®æ˜¯å¦‚ä½•æ±‚è§£æ®‹å·®å—çš„é›…å¯æ¯”è¡Œåˆ—å¼çš„å€¼ï¼Œé›…å¯æ¯”è¡Œåˆ—å¼å¯ä»¥è¡¨ç¤ºå¦‚ä¸‹ï¼š
$$
\begin{equation}
 \frac{\partial(x+G(x))}{\partial x}=I+\frac{\partial G}{\partial x} 
\end{equation}
$$
ä¸ºäº†æ±‚è§£è¯¥å¼ï¼Œ i-ResNet å…ˆåä½¿ç”¨**çº§æ•°å±•å¼€**ã€**æˆªæ–­**å’Œ**éšæœºè¿‘ä¼¼**ä¸‰ç§æ•°å­¦æ–¹æ³•ï¼šé¦–å…ˆç”¨æ’ç­‰å¼å°†é›…å¯æ¯”è¡Œåˆ—å¼ç»å¯¹å€¼çš„å¯¹æ•°**è½¬åŒ–ä¸ºæ±‚è¿¹**ï¼Œå¹¶åœ¨ä½¿ç”¨çº§æ•°å±•å¼€å½¢å¼ååœ¨ç¬¬ n é¡¹æˆªæ–­ï¼Œç„¶åä½¿ç”¨éšæœºè¿‘ä¼¼æ–¹æ³•å¾—åˆ°è¿‘ä¼¼å€¼ã€‚

i-ResNet ä½¿ç”¨å¤šç§æ‰‹æ®µç›´æ¥ä¸”è¾ƒé«˜æ•ˆçš„æ±‚è§£å‡ºæ®‹å·®å—çš„é›…å¯æ¯”è¡Œåˆ—å¼ï¼Œå°½ç®¡æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ä¸GLOW ç›¸å·®å¾ˆå¤§ï¼Œä½†æ‘†è„±äº†è€¦åˆå±‚çš„å¼Šç«¯ï¼Œæ˜¯å¯¹ FLOW æ¨¡å‹çš„é©æ–°å’Œå¤§èƒ†çš„å°è¯•



---

## 5.4 å˜åˆ†æ¨ç†æµ







---

## 5.5 æ€»ç»“

Flow æ˜¯ä¸€ä¸ªéå¸¸ç²¾å·§çš„æ¨¡å‹ï¼Œä¹Ÿæ˜¯åœ¨**ç†è®ºä¸Šæ²¡æœ‰è¯¯å·®çš„æ¨¡å‹**ã€‚Flow è®¾è®¡äº†ä¸€ä¸ªå¯é€†çš„ç¼–ç å™¨ï¼Œåªè¦è®­ç»ƒå‡ºç¼–ç å™¨çš„å‚æ•°å°±èƒ½ç›´æ¥å¾—åˆ°å®Œæ•´çš„è§£ç å™¨ï¼Œå®Œæˆç”Ÿæˆæ¨¡å‹çš„æ„é€ ã€‚ä¸ºäº†ä¿è¯ç¼–ç å™¨çš„å¯é€†
æ€§å’Œè®¡ç®—ä¸Šçš„å¯è¡Œæ€§ï¼Œç›®å‰ Flow ç±»æ¨¡å‹åªèƒ½ä½¿ç”¨å¤šä¸ªè€¦åˆå±‚çš„å †å æ¥å¢åŠ æ¨¡å‹çš„æ‹Ÿåˆèƒ½åŠ›ï¼Œä½†è€¦åˆå±‚çš„æ‹Ÿåˆèƒ½åŠ›æœ‰é™ï¼Œè¿™ç§æ–¹æ³•å¾ˆå¤§ç¨‹åº¦ä¸Šé™åˆ¶äº†æ¨¡å‹çš„æ€§èƒ½ã€‚ 

Flow ç›®å‰çš„åº”ç”¨èŒƒå›´é›†ä¸­åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸä¸­çš„äººè„¸ç”Ÿæˆï¼Œæœ€ä¼˜ç§€çš„æ¨¡å‹ä¸º GLOW ã€‚ç›¸æ¯”äºä»¥ GAN ä¸ºé¦–çš„å…¶ä»–æ·±åº¦ç”Ÿæˆæ¨¡å‹ï¼ŒFlow å‚æ•°é‡æ›´å¤šã€è¿ç®—é‡æ›´å¤§ï¼Œä¸”**åº”ç”¨é¢†åŸŸåªå±€é™äºå›¾åƒç”Ÿæˆ**ï¼Œè¿™äº›å¼Šç«¯é™åˆ¶äº† Flow çš„è¿›ä¸€æ­¥å‘å±•ï¼Œä½œä¸ºæ— è¯¯å·®çš„ç”Ÿæˆæ¨¡å‹ï¼Œæ½œåŠ›å·¨å¤§çš„ Flow æ¨¡å‹åº”è¯¥åœ¨æœªæ¥çš„ç ”ç©¶ä¸­**å¯»æ‰¾æ›´é«˜æ•ˆçš„å¯é€†ç¼–ç å™¨ç»“æ„æˆ–è€…æ‹Ÿåˆèƒ½åŠ›æ›´å¼ºçš„è€¦åˆå±‚ï¼Œå¹¶æ‰©å±•æ¨¡å‹çš„åº”ç”¨èŒƒå›´**ã€‚



https://scholar.google.com/scholar?start=40&hl=en&scisbd=1&as_sdt=2005&sciodt=0,5&cites=13099094504334344711&scipsc=

2021-07-28

**start**

Hierarchical Reinforcement Learning with Optimal Level Synchronization based on a Deep Generative Model JY Kim, [J Xuan](https://scholar.google.com/citations?user=POQ_yJUAAAAJ&hl=en&oi=sra), C Liang, [F Hussain](https://scholar.google.com/citations?user=L2Ve-R0AAAAJ&hl=en&oi=sra) - arXiv preprint arXiv:2107.08183, 2021 - arxiv.org

**end**

Implicit Normalizing Flows

2020å¹´10æœˆ7æ—¥ å°¾å·2021å‰å…¨éƒ¨æ•´ç†å®Œ



## 1. Hierarchical Reinforcement Learning with Optimal Level Synchronization based on a Deep Generative Model

Normalizing Flow : A simple distribution can be transformed into a complex distribution by repeatedly using an invertible mapping function. The change of the variable theorem makes the transformation from a variable to a new one possible and leads to the final distribution of the target variable as follows. Suppose a probability density function $z~q(z)$ for a random variable $z$. If an invertible bijective transformation function ğ‘” exists between a new variable $x$ and $y$, $x=f(z)$ and $z=f^{-1}(x)$. Again, if the change of the variable theorem is applied to $x$ and $z$ in the multivariate version,
$$
\begin{equation}
 \mathbf{z}_{i-1} \sim q_{i-1}\left(\mathbf{z}_{i-1}\right), \mathbf{z}_{i}=f_{i}\left(\mathbf{z}_{i-1}\right), \mathbf{z}_{i-1}=f_{i}^{-1}\left(\mathbf{z}_{i}\right) 
\end{equation}
$$
and then
$$
\begin{equation}
 q_{i}\left(\mathbf{z}_{i}\right)=q_{i-1}\left(f_{i}^{-1}\left(\mathbf{z}_{i}\right)\right)\left|\frac{d f_{i}^{-1}}{d \mathbf{z}_{i}}\right| \\
 \log q_{i}\left(\mathbf{z}_{i}\right)=\log q_{i-1}\left(\mathbf{z}_{i-1}\right)-\log \left|\frac{d f_{i}}{d \mathbf{z}_{i-1}}\right| 
\end{equation}
$$
Finally, the chain of $K$ transformations of probability density function $f_i$, which is easily inverted and whose Jacobian determinant can be easily computed, from the initial distribution $z_0$ yields a final target variable $x$,
$$
\begin{equation}
 \mathbf{x}=\mathbf{z}_{\mathrm{K}}=f_{\mathrm{K}} \circ \circ \circ f_{2} \circ f_{1}\left(\mathbf{z}_{0}\right) 
 \\
 \log p(\mathbf{x})=\log q_{\mathrm{K}}\left(\mathbf{z}_{\mathrm{K}}\right)=\log q_{0}\left(\mathbf{z}_{0}\right)-\sum_{i=1}^{\mathrm{K}} \log \left|\frac{d f_{i}}{d \mathbf{z}_{i-1}}\right| 
\end{equation}
$$
In our research, we focus on the advantages of a normalizing flow, which are model flexibility and generation speed, even though it also has drawbacks. A Real-valued Non-Volume
Preserving algorithm (RealNVP) makes use of a normalizing flow which is implemented with an invertible bijective transformation function. Each bijection called an affine coupling
layer, which is $ f: \mathbf{x} \mapsto \mathbf{y} $, decomposes an input dimension into two sections. The intrinsic transformation property using the affine coupling layer causes the input dimension to be unchanged with the alternate modification of the two split input sections in each coupling layer. Based on this property, the inverse operation is attained without difficulty. 

In addition, the inverse operation easily computes its Jacobian determinant since its Jacobian is a lower triangular matrix. RealNVP uses a multi-scale architecture as well as a batch normalization for better performance. To support a local correlation structure of an image, there are two masked convolution methods: the spatial checkerboard pattern mask and channel-wise mask[31]. Non-linear Independent Components Estimation (NICE) which is a previous model of RealNVP uses an additive coupling layer which does not use the scale term of an affine coupling layer [32]. Generative flow with 1Ã—1 convolutions (Glow) is a method to simplify the architecture regarding a reverse operation of channel ordering of NICE and RealNVP[33].



Several studies have tried to overcome the chronic drawback of a normalizing flow, biased log-density estimation [34], [35]. [36] utilizes the intrinsic characteristic of FDGM with an inductive bias based on a model architecture. Hence, the research takes advantage of a biased log-density estimation of FDGM itself. The research suggests a model architecture
using a VAE which extracts a global representation of an image and a FDGM, which depends on a local representation, with a conditional input of the global representation of the image.
Finally, an unbiased log-density estimation of an image can be expected from the FDGM using the output of the VAE. We adopt this architecture as the main idea for our model. The
model architecture is as follows. The compression encoder
$$
\begin{equation}
 q_{\phi}(z \mid x) 
\end{equation}
$$
in the VAE framework compresses the image $x$ with a high dimension to the latent representation $z$ with a low dimension. Then to reconstruct $x$ using a flow-based decoder 
$$
\begin{equation}
 v=f_{\theta}(x ; z) 
\end{equation}
$$
where a latent representation $z$, which is the output of the compression encoder, is fed into the flow-based decoder as a conditional input for dealing with the biased log-density estimation of FDGM. Finally, image $x$ is reconstructed by using the inverse function 
$$
\begin{equation}
 x=f_{\theta}^{-1}(v ; z) 
\end{equation}
$$


## 2. Continuous Latent Process Flows

Normalizing flows [ 32 , 12 , 23 , 13 , 29 , 22 , 3 , 8 , 24 , 30 ] employ a bijective mapping $ f: \mathbb{R}^{d} \rightarrow \mathbb{R}^{d} $ to transform a random variable $Y$ with a simple base distribution $p_Y$ to a random variable $X$ with a complex target distribution $p_X$ . We can sample from a normalizing flow by first sampling $y âˆ¼ p_Y$ and then transforming it to $x = f(y)$ . As a result of invertibility, normalizing flows can also be used for density estimation. Using the change-of-variables formula, we have ,$ \log p_{\boldsymbol{X}}(\boldsymbol{x})=  \log p_{\boldsymbol{Y}}(g(\boldsymbol{x}))+\log \mid \operatorname{det}\left(\frac{\partial g}{\partial \boldsymbol{x}}\right) $ where $g$ is the inverse of $f$.





## 3. Approximation capabilities of measure-preserving neural networks

2.2 Measure-preserving neural networks

We first briefly present existing measure-preserving neural networks as follows, including NICE [6] and RevNet [14]. 

NICE is an architecture to unsupervised generative modeling via learning a nonlinear bijective transformation between the data space and a latent space. 

The architecture is composed of a series of modules which take inputs $ \left(x_{1}, x_{2}\right) $ and produce outputs $ \left(\hat{x}_{1}, \hat{x}_{2}\right) $ according to the following additive coupling rules,
$$
\begin{equation}
 \hat{x}_{1}=x_{1}+f_{\text {net }}\left(x_{2}\right) \\
 \hat{x}_{2}=x_{2} 
\end{equation}\tag{3}
$$
Here, $ f_{\text {net }} $ is typically a neural network, $x_1$ and $x_2$ form a partition of the vector in each layer. Since the model is invertible and its Jacobian has unit determinant, the log-likelihood and its gradient can be tractably computed. 

As an alternative, the components of inputs can be reshuffled before separating them. Clearly, this architecture is imposed measure-preserving constraints. A similar architecture is used in the reversible residual network (RevNet) [14] which is a variant of ResNets [17] to avoid storing intermediate activations during backpropagation relied on the invertible architecture. In each module, the inputs are decoupled into $ \left(x_{1}, x_{2}\right) $ and the outputs
 $ \left(\hat{x}_{1}, \hat{x}_{2}\right) $  are produced by
$$
\begin{equation}
\hat{x}_{1}=x_{1}+f_{n e t_{1}}\left(x_{2}\right) \\
\hat{x}_{2}=x_{2}+f_{n e t_{2}}\left(\hat{x}_{1}\right)
\end{equation} \tag{4}
$$
Here, $f_{n e t_{1}}$, $f_{n e t_{2}}$ are trainable neural networks. It is observed that (4) is composed of two modules defined in (3) with the given reshuffling operation before the second module and also measure-preserving.

The architecture we investigate is analogous to RevNet but without reshuffling operations and using fixed dimension-splitting mechanisms in each layer. Let us begin by introducing the modules sets. Given integer $ D \geq s \geq 2 $ and control families $ \mathcal{N} \mathcal{N}^{D-s+1} $, $ \mathcal{N} \mathcal{N}^{s-1} $, denote
$$
\begin{equation}
 \mathcal{M}_{u p}=\left\{m: x \mapsto \hat{x} \mid \hat{x}[: s]=x[: s]+f_{n e t}(x[s:]), \hat{x}[s:]=x[s:], f_{n e t} \in \mathcal{N} \mathcal{N}^{D-s+1}\right\} \\
 \mathcal{M}_{l o w}=\left\{m: x \mapsto \hat{x} \mid \hat{x}[: s]=x[: s], \hat{x}[s:]=x[s:]+f_{n e t}(\hat{x}[: s]), f_{n e t} \in \mathcal{N} \mathcal{N}^{s-1}\right\} 
\end{equation}
$$
Subsequently, we define the collection of measure-preserving neural networks generated by $ \mathcal{M}_{u p} $ and $ \mathcal{M}_{low} $ as
$$
\begin{equation}
 \Psi=\bigcup_{N \geq 1}\left\{m_{N} \circ \cdots \circ m_{1} \mid m_{i} \in \mathcal{M}_{u p} \cup \mathcal{M}_{\text {low }}, 1 \leq i \leq N\right\} 
\end{equation}
$$
We are in fact aiming to show the approximation property of $ \Psi$.



---

## 4. Improving the expressiveness of neural vocoding with non-affine Normalizing Flows

NF transforms some D-dimensional real vector of continuous random variables $u$ into another D-dimensional real vector of continuous random variables $x$. Usually $u$ is sampled from a simple base distribution (for example Logistic) $ p_{u}(\mathbf{u}) $. In the vocoding task x corresponds to audio signal that follows a probability density $ p_{x}(\mathbf{x}) $. Conceptually, we can outline two blocks in the NF. One is the transformation function $T$, which has to be invertible and differentiable. The other is the conditioner neural network $c$ that predicts the parametrization $h$ for the transformation $T$.
$$
\begin{equation}
 \mathbf{x}=T(\mathbf{u} ; \mathbf{h}) \quad \mathbf{u}=T^{-1}(\mathbf{x} ; \mathbf{h}) \quad \mathbf{h}=c(\mathbf{u}) 
\end{equation}
$$
Given the invertible and differentiable nature of $T$, the density of $x$ is well-defined and can be obtainable by a change of variables:
$$
\begin{equation}
 p_{x}(\mathbf{x})=p_{u}(\mathbf{u})\left|\operatorname{det} J_{T}(\mathbf{u})\right|^{-1} 
\end{equation}
$$
The Jacobian$ J_{T}(\mathbf{u}) $ is $ D \times D $ matrix of all partial derivatives of $T$ over $u$. In this section, we discuss the merits and limitations of different NF architectures and outline our model design.



There are two major paradigms of training NF. One paradigm is to fit NF to the data with Maximum Likelihood Estimation (MLE) [14, 16, 17, 18]. In practice, it means that the model computes $T^{-1}$ during training and $T$ during the synthesis. Another paradigm assumes that we can evaluate the target data density, and we aim to train a NF to minimize the divergence loss. Commonly this is done with knowledge distillation [26], where the data density is estimated through a teacher network [13, 15]. A notable example of this training in the context of vocoding is the use of a high-quality Wavenet [5] to train a NF-based PW [13]. This paradigm for training and synthesis requires only the forward transformation $T$. In both paradigms, to train the model, we have to compute the Jacobian determinant, which typically costs $ O\left(D^{3}\right) $. However, in many practical applications, we can reduce this complexity. 

An autoregressive conditioner network has the Jacobian that is a lower triangular matrix with determinant computable in $ O\left(D\right) $ [13, 14, 15, 16, 27, 28]. It is shown [24, 29] that under the
assumption of enough capacity and data, an autoregressive conditioner with non-linear transformations can approximate any continuous distribution with any desired precision - a property called universal approximation. NF using such a conditioner can parallelize the forward transformation computation,but its inverse is sequential. This poses a challenge for the MLE paradigm training due to the high temporal resolution of speech data. Coupling layers are a common workaround for this problem [14, 28, 30]. Such an architecture allows efficient computation of both forward and inverse transformations. However, it may limit the expressivity of NF since a significant number of dimensions are left unchanged at each flow layer [31]. Because of the above argumentation, in this work, we decided to use Parallel Wavenet [13] which is a fully-autoregressive model trained with knowledge distillation that does not require any computation of the transformation inverse.

The NF transformation has to be invertible and differentiable. The most straightforward and common design choice is to implement the transformation as an affine function [13, 14,
15, 16, 27, 28]. Such a design is attractive because of its simplicity and analytically tractability. However, the drawback of such a transformation is its limited expressivity. Specifically,
the output of NF belongs to the same distribution family as its base. In some cases, this might negatively affect the capture of multimodal target distributions [23, 24, 25]. To overcome this limitation, the transformation might be implement as a composition or the weighted sum of monotonically increasing activation functions[23,24,25], the integral of some positive function[29], or a spline of analytically invertible monotonic functions
[22, 30, 32]. All above transformations are Universal Approximators [24, 29]. Another idea is to use constrained residual functions [33, 34]. Unfortunately, for these methods, we either cannot efficiently compute the determinant of the Jacobian or the function has limited expressivity [31]. Finally, we might also construct the flow by defining an ordinary differential equation (ODE) that describes the evolution of NF in time instead of a finite sequence of transformations [17, 18]. According to recent surveys [35], Normalizing Flows with finite composition of non-affine transformations outperform other flow-based methods. Considering the above pros and cons, we decide to enhance PW with a composition of monotonically increasing non-affine activation functions inspired by Flow++ [25].







## *5. Large-capacity Image Steganography Based on Invertible Neural Networks

åŸºäºå¯é€†ç¥ç»ç½‘ç»œçš„å¤§å®¹é‡å›¾åƒéšå†™æŠ€æœ¯ï¼ˆä¸­æ–‡ç¿»è¯‘ç‰ˆï¼‰



å‚è€ƒæ„ä¹‰è¾ƒå¤§ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼

**Invertible Neural Networks (INN)**

Pioneering research on INN-based mapping can be seen in NICE [14] and RealNVP [15]. In [20] a further explanation for the invertibility is explored. INNs have also been proved to share some advantages in **estimating the posterior of an inverse problem** [2]. In [47], flexible INNs are constructed with masked convolutions under some composition rules. An **unbiased flow-based generative model** is also introduced in [13]. Besides that, FFJORD [21], Glow [34], i-RevNet [32] and i-ResNet [6] further improve the coupling layer for density estimation, achieving better generation results. 

Because of the powerful network representation, INNs are also used for **various inference tasks**, such as image colorization [3], image rescaling [58], image compression [54], and video super-resolution [64]. We take the advantage of INNâ€™s bijective construction and efficient invertibility for our steganography issue.



æ­£å‘é€†å‘å›¾å‚è€ƒ



![1627392034445](assets/1627392034445.png)

**Fig .2** System pipeline. Unlike traditional methods (a) where steganography and recovery of the hidden image are processed separately, we introduce an invertible steganography framework (b). The multiple hidden images are concatenated with the host image, serving as a forward input to the trainable invertible network. The container image is then generated using several invertible blocks sharing the same structures. Conversely, the backpropagation effectively recovers the hidden images with high quality from the container image.



## 6. Copula-Based Normalizing Flows

Density estimation via NFs revolve around learning a diffeomorphic transformation $T_Î¸$ that maps some unknown target distribution $P_x$ to a known and tractable base distribution
$P_z$ . At the cornerstone of NFs is the change of variables formula
$$
\begin{equation}
 p_{\theta}(x)=p_{\mathbf{z}}\left(T_{\theta}(x)\right)\left|\operatorname{det} J_{T_{\theta}}(x)\right| \quad for  x \in \mathbb{R}^{D} 
\end{equation}\tag{1}
$$
which relates the evaluation of the estimated density $p_Î¸$ of $ \mathbf{x} \sim P_{\mathbf{x}} $ to the evaluation of the base density $p_z$ , of $T_Î¸ (x)$ , and of $ \operatorname{det} J_{T_{\theta}}(x) $ . By composing simple diffeomorphic
building blocks $ T_{\theta}:=T_{\theta, l} \circ \cdots \circ T_{\theta, 1} $ , we are able to obtain expressive transformations, while presuming diffeomorphy and computational tractablity of the building blocks. Due to
the tractable PDF in (1) , we are able to train the model via maximum likelihood estimation (MLE)
$$
\begin{equation}
 \hat{\theta} \in \underset{\theta}{\arg \min } \mathbb{E}_{p_{\text {data }}}\left[-\log p_{\theta}(\mathbf{x})\right] 
\end{equation}
$$
where $p_{data}$ is the **PDF** of the empirical distribution of $x$ . A comprehensive overview of NFs, including the exact parameterizations of certain flow models $T_Î¸$ , computational aspects, and more, can be found in Kobyzev et al. (2020) and Papamakarios et al. (2021).



---

## *7. Densely connected normalizing flows

One of the uttermost goals of artificial intelligence is to generate images, audio waveforms, and natural language symbols. To achieve the desired goal, the current state of the art uses deep compositions of non-linear transformations [ 1 , 2 ] known as deep generative models [ 3 , 4 , 5 , 6 , 7 ]. Formally, deep generative models **estimate an unknown data distribution** $p_D$ given by a set of i.i.d. samples $ D=\left\{\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{n}\right\} $. The data distribution is approximated with a model distribution $p_Î¸$ defined by the architecture of the model and a set of parameters $Î¸$ . While the architecture is usually handcrafted, the set of parameters $Î¸$ is obtained by optimizing the likelihood across the training distribution $p_D$ :
$$
\begin{equation}
 \theta^{*}=\underset{\theta \in \Theta}{\operatorname{argmin}} \mathbb{E}_{\boldsymbol{x} \sim p_{D}}\left[-\ln p_{\theta}(\boldsymbol{x}) \right]
\end{equation}
$$
normalizing flows [ 6 , 17 , 18 ] model the likelihood **using a bijective mapping** to a predefined latent distribution $p(z)$ , typically a multivariate Gaussian. Given the bijection $f_Î¸$ , the likelihood is defined using the change of variables formula:
$$
\begin{equation}
 p_{\theta}(\boldsymbol{x})=p(\boldsymbol{z})\left|\operatorname{det} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{x}}\right|, \quad \boldsymbol{z}=f_{\theta}(\boldsymbol{x}) 
\end{equation}\tag{2}
$$
This approach requires computation of the Jacobian determinant ($ \operatorname{det} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{x}} $). Therefore, during the construction of bijective transformations, a great emphasis is placed on **tractable determinant computation** and **efficient inverse computation** [RealNVP, Glow]. Due to these constraints, invertible transformations require more parameters to achieve a similar capacity compared to standard NN building blocks [i-revnet].Still, modeling $p_Î¸(x)$ using bijective formulation enables exact likelihood evaluation and efficient sample generation, which makes this approach convenient for various downstream tasks [21 , 22 , 23].

The bijective formulation (2) implies that the input and the latent representation have the same dimensionality. Typically, convolutional units of normalizing-flow approaches [RealNVP] **internallyå†…éƒ¨åœ° inflateè†¨èƒ€** the dimensionality of the input, extract useful features, and then **compresså‹ç¼© them back** to the original dimensionality. Unfortunately, the capacity of such transformations is limited by input dimensionality[Vflow]. This issue can be addressed by expressing the model as a sequence of bijective transformations [RealNVP]. However, increasing the depth alone is a suboptimal approach to improve capacity of a deep model [Efficientnet]. Recent works propose to widen the flow by increasing the input dimensionality [Vflow, Augmented normalizing flows]. We propose an effective development of that idea which further improves the performance while relaxing computational requirements.

We increase the expressiveness of normalizing flows by incremental augmentation of intermediate latent representations with Gaussian noise. The proposed cross-unit coupling applies an affine transformation to the noise, where the scaling and translation are computed from a set of previous intermediate representations. In addition, we improve intra-unit coupling by proposing a transformation which fuses the global spatial context with local correlations. The proposed image-oriented architecture improves expressiveness and computational efficiency. Our models set the new state-of-the-art result in likelihood evaluation on ImageNet32, ImageNet64 and CelebA.



**7.2.1. Normalizing flows with cross-unit coupling**







---

## 8. On the expressivity of bi-Lipschitz normalizing flows

Residual flow

**2.Background**

**2.1. Bi-Lipschitz Normalizing Flows**





----

## 9. PassFlow: Guessing Passwords with Generative
Flows

2.Background





---

## 10. Normalizing Flows for Probabilistic Modeling and Inference

**3.3 Residual Flows**





---

## 11. Data-efficient meta-learning with Bayesian

**2.8 normalize flow**





---

## 12. Invertible DenseNets with Concatenated LipSwish

**2 Background**





---

## 13. TRUMPETS : Injective Flows for Inference and Inverse Problems

Residual flow

**3.3 ESTIMATING LOG-LIKELIHOODS**





----

## 14. CONVEX POTENTIAL FLOWS

**3.2 O(1)-M EMORY U NBIASED âˆ‡logdetH ESTIMATOR**





## 15. Multiscale Invertible Generative Networks for High-Dimensional Bayesian Inference

å¤šå°ºåº¦å¤„ç†